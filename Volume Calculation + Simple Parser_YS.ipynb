{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd7f833",
   "metadata": {},
   "source": [
    "Author: Yibing Sun\n",
    "Project: Visual Hate\n",
    "Data: Tweets that incorporate hate slurs and have attachments\n",
    "Share to: CAMER, SJMC, UW-Madison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5bd792",
   "metadata": {},
   "source": [
    "I usually share this when I’m receiving requests about sharing scripts: \n",
    "    \n",
    "The scripts that I have written are usually created for the particular project. \n",
    "If you are going to use the scripts for your other projects, please be respectful to the original creator (me or others). \n",
    "Ask for permission before you use it. You can either ask for interests in co-authoring or acknowledging their contribution in the publication stage. I do the same thing to acknowledge other people’s efforts as well. Thank you for your understanding! \n",
    "    \n",
    "Yibing Sun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18af1279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4684ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_json_files(filenames, output_filename):\n",
    "    data = []\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('json'):\n",
    "            with open(filename, \"r\") as file:\n",
    "                file_data = json.load(file)\n",
    "                data.append(file_data)\n",
    "    with open(output_filename, \"w\") as outfile:\n",
    "        json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfa22ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames_in_folder(folder_path):\n",
    "    return [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80757dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of subfolders for a specific month\n",
    "file_path = \"YOUR PATH\"\n",
    "folder_list = os.listdir(\"YOUR PATH\")\n",
    "subf_list = []\n",
    "for i in range(0, len(folder_list)):\n",
    "    subf_list.append(os.path.join(file_path, folder_list[i]))\n",
    "subf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac42a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of files within that day\n",
    "for folder in subf_list:\n",
    "    filenames = get_filenames_in_folder(folder)\n",
    "#     filenames = filenames[:]\n",
    "    print(filenames)\n",
    "    day = filenames[0].split('_')[0]\n",
    "#     print(filenames)\n",
    "    filename_list = [folder+'/' + filename for filename in filenames]\n",
    "#     print(filename_list)\n",
    "    concatenate_json_files(filename_list, \"YOUR OWN PATH WITH FORMAT\".format(day)) \n",
    "    print(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbffcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames_in_folder(folder_path):\n",
    "    return [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7efb86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_filenames_in_folder('YOUR PATH')\n",
    "filenames_json = []\n",
    "for f in test:\n",
    "    if f.endswith('json'):\n",
    "        filenames_json.append(f)\n",
    "filenames_json = ['PART OF YOUR FORMATTED PATH' + x for x in filenames_json]\n",
    "filenames_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0c1ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(columns = ['Date', 'Tweets_Volume', 'Retweet_Count','Reply_Count',\n",
    "                                 'Like_count', 'Quote_count','Sensitive', 'Retweet_N', 'Retweet_De'],\n",
    "                      index=range(YOUR CHOSEN NUMBER))\n",
    "for i in range(len(filenames_json)):\n",
    "    tweets_res = []\n",
    "    for line in open(filenames_json[i], encoding = \"ISO-8859-1\"):\n",
    "        try:\n",
    "            tweets_res.append(json.loads(line))\n",
    "        except ValueError as e:\n",
    "            tweets_res.append(\"Error\")\n",
    "            print(e)\n",
    "            print(line)\n",
    "    tweet_flat = []\n",
    "    for s in tweets_res:\n",
    "        for j in s:\n",
    "            if j.get('data'):\n",
    "                for k in j['data']:\n",
    "                    tweet_flat.append(k)\n",
    "    df = pd.DataFrame(tweet_flat)\n",
    "    df = df.sort_values(\"created_at\")\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    #create another column referenced id for future vlookup\n",
    "    df['referenced_id'] = df['referenced_tweets']\n",
    "    for i1 in range(len(df)):\n",
    "        if type(df.iloc[i1, -1]) != float:\n",
    "            df.iloc[i1, -1] = df.iloc[i1, -1][0]['id']\n",
    "\n",
    "    ### Dataframe 2: retweet entities\n",
    "    retweet_flat = []\n",
    "    for tweet in tweets_res:\n",
    "        for j1 in tweet:\n",
    "            if j1.get('includes').get('tweets'):\n",
    "                for k1 in j1['includes']['tweets']:\n",
    "                    retweet_flat.append(k1)\n",
    "    df1 = pd.DataFrame(retweet_flat)\n",
    "    df1=df1.rename(columns = {'id':'referenced_id', 'text': 'referenced_text', 'attachments': 'referenced_attachment'})\n",
    "    df1 = df1[['referenced_id','referenced_text', 'referenced_attachment']]\n",
    "    Retweet_N = df1.shape[0]\n",
    "    df1['referenced_attachment'] = df1['referenced_attachment'].astype(str)\n",
    "    df1 = df1.drop_duplicates()\n",
    "    Retweet_De = df1.shape[0]\n",
    "\n",
    "    df3 = pd.merge(df, df1, on=\"referenced_id\", how = \"left\")\n",
    "\n",
    "    prc = pd.json_normalize(df3.public_metrics, sep=',')\n",
    "    test = df3.join(prc)\n",
    "    Tweets_Volume = test.shape[0]\n",
    "\n",
    "    Retweet_Count = round(test[\"retweet_count\"].mean(),2)\n",
    "    Reply_Count = round(test[\"reply_count\"].mean(),2)\n",
    "    Like_count = round(test[\"like_count\"].mean(), 2)\n",
    "    Quote_count = round(test[\"quote_count\"].mean(), 2)\n",
    "    try:\n",
    "        Sensitive = test['possibly_sensitive'].value_counts()[1]\n",
    "    except:\n",
    "        Sensitive = 0\n",
    "    \n",
    "\n",
    "    name = filenames_json[i].split('/')[-1].split('.')[0]\n",
    "    test.to_csv('YOUR FORMATTED PATH'.format(name), index = False)\n",
    "    print(name)\n",
    "    \n",
    "    output.iloc[i,0] = name\n",
    "    output.iloc[i,1] = Tweets_Volume\n",
    "    output.iloc[i,2] = Retweet_Count\n",
    "    output.iloc[i,3] = Reply_Count\n",
    "    output.iloc[i,4] = Like_count\n",
    "    output.iloc[i,5] = Quote_count\n",
    "    output.iloc[i,6] = Sensitive\n",
    "    output.iloc[i,7] = Retweet_N\n",
    "    output.iloc[i,8] = Retweet_De"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc676bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db65c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('YOUR PATH/summary.csv') #Summarized csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
